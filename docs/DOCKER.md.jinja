{# Extract values from preset #}
{% if pytorch_cuda_preset == 'pytorch-2.9.0-cuda-12.6' %}
  {% set pytorch_version = '2.9.0' %}
  {% set cuda_version = '12.6' %}
  {% set cudnn_version = '9.16.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.9.0-cuda-13.0' %}
  {% set pytorch_version = '2.9.0' %}
  {% set cuda_version = '13.0' %}
  {% set cudnn_version = '9.16.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.8.0-cuda-12.6' %}
  {% set pytorch_version = '2.8.0' %}
  {% set cuda_version = '12.6' %}
  {% set cudnn_version = '9.16.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.8.0-cuda-12.8' %}
  {% set pytorch_version = '2.8.0' %}
  {% set cuda_version = '12.8' %}
  {% set cudnn_version = '9.16.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.7.1-cuda-12.6' %}
  {% set pytorch_version = '2.7.1' %}
  {% set cuda_version = '12.6' %}
  {% set cudnn_version = '9.16.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.7.1-cuda-11.8' %}
  {% set pytorch_version = '2.7.1' %}
  {% set cuda_version = '11.8' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.6.0-cuda-12.4' %}
  {% set pytorch_version = '2.6.0' %}
  {% set cuda_version = '12.4' %}
  {% set cudnn_version = '9.1.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.6.0-cuda-11.8' %}
  {% set pytorch_version = '2.6.0' %}
  {% set cuda_version = '11.8' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.5.1-cuda-12.4' %}
  {% set pytorch_version = '2.5.1' %}
  {% set cuda_version = '12.4' %}
  {% set cudnn_version = '9.1.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.5.1-cuda-12.1' %}
  {% set pytorch_version = '2.5.1' %}
  {% set cuda_version = '12.1' %}
  {% set cudnn_version = '9.1.0' %}
{% elif pytorch_cuda_preset == 'pytorch-2.5.1-cuda-11.8' %}
  {% set pytorch_version = '2.5.1' %}
  {% set cuda_version = '11.8' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.4.1-cuda-12.4' %}
  {% set pytorch_version = '2.4.1' %}
  {% set cuda_version = '12.4' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.4.1-cuda-12.1' %}
  {% set pytorch_version = '2.4.1' %}
  {% set cuda_version = '12.1' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.4.1-cuda-11.8' %}
  {% set pytorch_version = '2.4.1' %}
  {% set cuda_version = '11.8' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.3.1-cuda-12.1' %}
  {% set pytorch_version = '2.3.1' %}
  {% set cuda_version = '12.1' %}
  {% set cudnn_version = '8.9.7' %}
{% elif pytorch_cuda_preset == 'pytorch-2.3.1-cuda-11.8' %}
  {% set pytorch_version = '2.3.1' %}
  {% set cuda_version = '11.8' %}
  {% set cudnn_version = '8.9.7' %}
{% else %}
  {% set pytorch_version = pytorch_version %}
  {% set cuda_version = cuda_version %}
  {% set cudnn_version = cudnn_version %}
{% endif %}
# Dockerç’°å¢ƒã‚¬ã‚¤ãƒ‰ï¼ˆGPUå¯¾å¿œï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€GPUï¼ˆNVIDIA CUDAï¼‰å¯¾å¿œã®é–‹ç™ºç”¨ã¨æœ¬ç•ªç”¨ã®2ç¨®é¡ã®Dockerç’°å¢ƒãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚

**ç’°å¢ƒæ§‹æˆ**: PyTorch {{ pytorch_version }} + CUDA {{ cuda_version }} + cuDNN {{ cudnn_version }}

## ğŸ“‹ ç›®æ¬¡

- [æ¦‚è¦](#æ¦‚è¦)
- [å‰ææ¡ä»¶](#å‰ææ¡ä»¶)
- [é–‹ç™ºç’°å¢ƒï¼ˆDockerfile.devï¼‰](#é–‹ç™ºç’°å¢ƒdockerfiledev)
- [æœ¬ç•ªç’°å¢ƒï¼ˆDockerfile.prodï¼‰](#æœ¬ç•ªç’°å¢ƒdockerfileprod)
- [docker-composeã®ä½¿ã„æ–¹](#docker-composeã®ä½¿ã„æ–¹)
- [GPUå‹•ä½œç¢ºèª](#gpuå‹•ä½œç¢ºèª)
- [ã‚ˆãã‚ã‚‹è³ªå•](#ã‚ˆãã‚ã‚‹è³ªå•)

---

## æ¦‚è¦

### é–‹ç™ºç’°å¢ƒ vs æœ¬ç•ªç’°å¢ƒ

| é …ç›® | é–‹ç™ºç’°å¢ƒï¼ˆdevï¼‰ | æœ¬ç•ªç’°å¢ƒï¼ˆprodï¼‰ |
|------|----------------|-----------------|
| **ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ¡ãƒ¼ã‚¸** | `nvidia/cuda:{{ cuda_version }}-cudnn{{ cudnn_version.split('.')[0] }}-devel-ubuntu22.04` | `nvidia/cuda:{{ cuda_version }}-cudnn{{ cudnn_version.split('.')[0] }}-runtime-ubuntu22.04` |
| **CUDAãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ** | é–‹ç™ºç”¨ï¼ˆnvccå«ã‚€ï¼‰ | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã¿ |
| **ä¾å­˜é–¢ä¿‚** | å…¨ä¾å­˜é–¢ä¿‚ï¼ˆdevå«ã‚€ï¼‰ | æœ¬ç•ªä¾å­˜é–¢ä¿‚ã®ã¿ |
| **ãƒ“ãƒ«ãƒ‰æ–¹å¼** | ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸ | ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆæœ€é©åŒ–ï¼‰ |
| **ã‚³ãƒ¼ãƒ‰ãƒã‚¦ãƒ³ãƒˆ** | âœ… ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰å¯¾å¿œ | âŒ ã‚¤ãƒ¡ãƒ¼ã‚¸ã«å›ºå®š |
| **SSHå¯¾å¿œ** | âœ… ãƒãƒ¼ãƒˆ2222 | âŒ |
| **Jupyter Lab** | âœ… ãƒãƒ¼ãƒˆ8888 | âŒ |
| **ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚µã‚¤ã‚º** | å¤§ãã„ï¼ˆ~8GBï¼‰ | å°ã•ã„ï¼ˆ~5GBï¼‰ |
| **ç”¨é€”** | ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚° | ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»æ¨è«–ã‚µãƒ¼ãƒãƒ¼ |

{% if use_nix %}
### Nixç’°å¢ƒã¨ã®ä½¿ã„åˆ†ã‘

- **Nix (æ¨å¥¨)**: ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã€å†ç¾æ€§é‡è¦–ã€è¤‡æ•°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†
- **Docker**: ãƒ‡ãƒ—ãƒ­ã‚¤ã€CI/CDã€ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã€ãƒãƒ¼ãƒ å…±æœ‰ã€Nixéå¯¾å¿œç’°å¢ƒ

**ãƒã‚¤ãƒ³ãƒˆ**: `uv.lock` ãŒä¸¡ç’°å¢ƒã®ä¾å­˜é–¢ä¿‚ã‚’ä¿è¨¼ã—ã¾ã™ã€‚
{% endif %}

---

## å‰ææ¡ä»¶

### å¿…é ˆãƒ„ãƒ¼ãƒ«

1. **Docker Desktop** ã¾ãŸã¯ **Docker Engine**
   ```bash
   # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
   docker --version
   docker compose version
   ```

2. **NVIDIA Container Toolkit** (GPUä½¿ç”¨ã®ãŸã‚å¿…é ˆ)

   ```bash
   # Ubuntu/Debian ã®å ´åˆ
   distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
   curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
   curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
     sudo tee /etc/apt/sources.list.d/nvidia-docker.list

   sudo apt-get update
   sudo apt-get install -y nvidia-container-toolkit
   sudo systemctl restart docker
   ```

   **å‹•ä½œç¢ºèª**:
   ```bash
   docker run --rm --gpus all nvidia/cuda:{{ cuda_version }}-base-ubuntu22.04 nvidia-smi
   ```

   æ­£å¸¸ã«å‹•ä½œã™ã‚Œã°ã€GPUã®æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

3. **ãƒ›ã‚¹ãƒˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID/ã‚°ãƒ«ãƒ¼ãƒ—ID** (é–‹ç™ºç’°å¢ƒç”¨)
   ```bash
   # ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆæ¨å¥¨ï¼‰
   export USER_ID=$(id -u)
   export GROUP_ID=$(id -g)
   export USERNAME=$(whoami)
   ```

---

## é–‹ç™ºç’°å¢ƒï¼ˆDockerfile.devï¼‰

### ç‰¹å¾´

- âœ… **GPUé–‹ç™ºãƒ„ãƒ¼ãƒ«**: nvccï¼ˆCUDA Compilerï¼‰å«ã‚€
- âœ… **ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰**: ãƒ›ã‚¹ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚¦ãƒ³ãƒˆã€å¤‰æ›´ãŒå³åæ˜ 
- âœ… **Jupyter Labå¯¾å¿œ**: ãƒãƒ¼ãƒˆ8888ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
- âœ… **SSHå¯¾å¿œ**: ãƒªãƒ¢ãƒ¼ãƒˆé–‹ç™ºç’°å¢ƒã¨ã—ã¦åˆ©ç”¨å¯èƒ½
- âœ… **å…¨é–‹ç™ºãƒ„ãƒ¼ãƒ«**: {% if use_ruff %}Ruffã€{% endif %}{% if use_ty %}tyã€{% endif %}{% if use_pytest %}Pytest{% endif %}ãªã©å…¨ã¦å«ã‚€
- âœ… **uvã‚­ãƒ£ãƒƒã‚·ãƒ¥æ°¸ç¶šåŒ–**: ä¾å­˜é–¢ä¿‚ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é«˜é€ŸåŒ–

### ãƒ“ãƒ«ãƒ‰

```bash
# Docker Composeã‚’ä½¿ã†å ´åˆï¼ˆæ¨å¥¨ï¼‰
docker compose build dev

# ç›´æ¥ãƒ“ãƒ«ãƒ‰ã™ã‚‹å ´åˆ
docker build -f Dockerfile.dev -t {{ project_name }}-dev \
  --build-arg USERNAME=$(whoami) \
  --build-arg USER_ID=$(id -u) \
  --build-arg GROUP_ID=$(id -g) \
  --build-arg CUDA_VERSION={{ cuda_version }} \
  --build-arg CUDNN_MAJOR={{ cudnn_version.split('.')[0] }} \
  .
```

### å®Ÿè¡Œ

```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚§ãƒ«ï¼ˆGPUåˆ©ç”¨ï¼‰
docker compose run --rm dev bash

# PyTorchã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
docker compose run --rm dev python train.py

# GPUå‹•ä½œç¢ºèª
docker compose run --rm dev python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
{% if use_pytest %}
docker compose run --rm dev pytest
{% else %}
docker compose run --rm dev python -m unittest
{% endif %}

# Jupyter Labèµ·å‹•
docker compose run --rm -p 8888:8888 dev jupyter lab --ip=0.0.0.0 --no-browser

# TensorBoardèµ·å‹•
docker compose run --rm -p 6006:6006 dev tensorboard --logdir=runs --host=0.0.0.0
```

### SSHçµŒç”±ã§ã®ã‚¢ã‚¯ã‚»ã‚¹

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
docker compose up -d dev

# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§SSHãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’èµ·å‹•
docker compose exec dev sudo /usr/sbin/sshd

# SSHå…¬é–‹éµã‚’é…ç½®
docker compose exec -u $(whoami) dev mkdir -p ~/.ssh
docker compose cp ~/.ssh/id_rsa.pub $(docker compose ps -q dev):/home/$(whoami)/.ssh/authorized_keys
docker compose exec -u $(whoami) dev chmod 600 ~/.ssh/authorized_keys

# SSHã§ã‚¢ã‚¯ã‚»ã‚¹
ssh -p 2222 $(whoami)@localhost
```

---

## æœ¬ç•ªç’°å¢ƒï¼ˆDockerfile.prodï¼‰

### ç‰¹å¾´

- âš¡ **æœ€å°ã‚¤ãƒ¡ãƒ¼ã‚¸**: ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰ã§æœ€é©åŒ–ï¼ˆCUDA runtimeã®ã¿ï¼‰
- ğŸ”’ **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**: érootãƒ¦ãƒ¼ã‚¶ãƒ¼ã€read-only filesystem
- ğŸ“¦ **è‡ªå·±å®Œçµ**: ä¾å­˜é–¢ä¿‚ã¨ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦å«ã‚€
- ğŸš€ **æ¨è«–æœ€é©åŒ–**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ„ãƒ¼ãƒ«ä¸è¦
- â™»ï¸ **è‡ªå‹•å†èµ·å‹•**: `restart: unless-stopped` è¨­å®šæ¸ˆã¿

### ãƒ“ãƒ«ãƒ‰

```bash
# Docker Composeã‚’ä½¿ã†å ´åˆï¼ˆæ¨å¥¨ï¼‰
docker compose build prod

# ç›´æ¥ãƒ“ãƒ«ãƒ‰ã™ã‚‹å ´åˆ
docker build -f Dockerfile.prod -t {{ project_name }}-prod \
  --build-arg USERNAME=appuser \
  --build-arg CUDA_VERSION={{ cuda_version }} \
  --build-arg CUDNN_MAJOR={{ cudnn_version.split('.')[0] }} \
  --build-arg PYTHON_VERSION={{ python_version }} \
  .
```

### å®Ÿè¡Œ

```bash
# ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
docker compose up prod

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
docker compose up -d prod

# ãƒ­ã‚°ç¢ºèª
docker compose logs -f prod

# åœæ­¢
docker compose down
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒãƒ³ãƒ‰

```bash
# æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
docker run --rm --gpus all {{ project_name }}-prod python inference.py --input data.pt

# GPUå‹•ä½œç¢ºèª
docker run --rm --gpus all {{ project_name }}-prod python -c "import torch; print(torch.cuda.is_available())"

# ç’°å¢ƒå¤‰æ•°ã‚’æ¸¡ã™
docker run --rm --gpus all -e MODEL_PATH=/models/best.pt {{ project_name }}-prod python inference.py

# ãƒãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆAPIã‚µãƒ¼ãƒãƒ¼ã®å ´åˆï¼‰
docker run --rm --gpus all -p 8080:8000 {{ project_name }}-prod python server.py
```

---

## docker-composeã®ä½¿ã„æ–¹

### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰

```bash
# ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§
docker compose ps

# ãƒ“ãƒ«ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
docker compose build --no-cache dev

# ãƒ­ã‚°ç¢ºèª
docker compose logs -f dev

# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker compose exec dev bash

# åœæ­¢ã—ã¦å‰Šé™¤
docker compose down

# ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚‚å‰Šé™¤
docker compose down -v
```

### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã« `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼š

```bash
# .env
USERNAME=myuser
USER_ID=1000
GROUP_ID=1000
```

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

`docker-compose.override.yml` ã‚’ä½œæˆã—ã¦è¨­å®šã‚’ä¸Šæ›¸ãï¼š

```yaml
# docker-compose.override.yml
services:
  dev:
    ports:
      - "8889:8888"  # Jupyter Labãƒãƒ¼ãƒˆå¤‰æ›´
    environment:
      - CUDA_VISIBLE_DEVICES=0,1  # ä½¿ç”¨ã™ã‚‹GPUã‚’åˆ¶é™
    shm_size: '16gb'  # å…±æœ‰ãƒ¡ãƒ¢ãƒªå¢—é‡
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']  # GPU 0,1ã®ã¿ä½¿ç”¨
              capabilities: [gpu]
```

---

## GPUå‹•ä½œç¢ºèª

### åŸºæœ¬ãƒã‚§ãƒƒã‚¯

```bash
# ãƒ›ã‚¹ãƒˆã§GPUç¢ºèª
nvidia-smi

# Dockerå†…ã§GPUç¢ºèª
docker run --rm --gpus all nvidia/cuda:{{ cuda_version }}-base-ubuntu22.04 nvidia-smi

# PyTorchã§CUDAç¢ºèª
docker compose run --rm dev python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA version: {torch.version.cuda}')
print(f'GPU count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
```

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```bash
# ç°¡æ˜“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
docker compose run --rm dev python -c "
import torch
import time

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# è¡Œåˆ—ä¹—ç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
size = 5000
a = torch.randn(size, size, device=device)
b = torch.randn(size, size, device=device)

# ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
_ = torch.matmul(a, b)

# è¨ˆæ¸¬
torch.cuda.synchronize()
start = time.time()
c = torch.matmul(a, b)
torch.cuda.synchronize()
end = time.time()

print(f'{size}x{size} matrix multiplication: {end-start:.4f} seconds')
print(f'TFLOPS: {2*size**3/(end-start)/1e12:.2f}')
"
```

---

## ã‚ˆãã‚ã‚‹è³ªå•

### Q1: Nixç’°å¢ƒã¨Dockerç’°å¢ƒã§ä¾å­˜é–¢ä¿‚ãŒç•°ãªã‚‹ã“ã¨ã¯ã‚ã‚‹ï¼Ÿ

A: **ã‚ã‚Šã¾ã›ã‚“**ã€‚`uv.lock` ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸¡ç’°å¢ƒã§å…±æœ‰ã•ã‚Œã‚‹ãŸã‚ã€å…¨ãåŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¾å­˜é–¢ä¿‚ï¼ˆPyTorch {{ pytorch_version }}ã€CUDA {{ cuda_version }} å¯¾å¿œï¼‰ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¾ã™ã€‚

### Q2: GPUãŒèªè­˜ã•ã‚Œãªã„

A: ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. **NVIDIA Container ToolkitãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹**:
   ```bash
   docker run --rm --gpus all nvidia/cuda:{{ cuda_version }}-base-ubuntu22.04 nvidia-smi
   ```

2. **docker-compose.ymlã§GPUè¨­å®šãŒã‚ã‚‹ã‹**:
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: all
             capabilities: [gpu]
   ```

3. **ãƒ›ã‚¹ãƒˆã§GPUãŒå‹•ä½œã—ã¦ã„ã‚‹ã‹**:
   ```bash
   nvidia-smi
   ```

4. **Dockerã®å†èµ·å‹•**:
   ```bash
   sudo systemctl restart docker
   ```

### Q3: "out of memory" ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹

A: ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š

1. **å…±æœ‰ãƒ¡ãƒ¢ãƒªï¼ˆshm_sizeï¼‰ã‚’å¢—ã‚„ã™**:
   ```yaml
   # docker-compose.yml
   shm_size: '16gb'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯8gb
   ```

2. **ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹**:
   ```python
   # train.py
   batch_size = 32  # 64ã‹ã‚‰32ã«å¤‰æ›´
   ```

3. **ä½¿ç”¨ã™ã‚‹GPUã‚’åˆ¶é™**:
   ```bash
   export CUDA_VISIBLE_DEVICES=0  # GPU 0ã®ã¿ä½¿ç”¨
   docker compose run --rm dev python train.py
   ```

4. **æ··åˆç²¾åº¦å­¦ç¿’ã‚’ä½¿ç”¨**:
   ```python
   # PyTorch AMP
   from torch.cuda.amp import autocast, GradScaler
   ```

### Q4: é–‹ç™ºç’°å¢ƒã§ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ãŒåæ˜ ã•ã‚Œãªã„

A: ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
1. `docker-compose.yml` ã§ `.:/app` ãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹
2. ã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ï¼ˆ`docker compose ps`ï¼‰
3. Python ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ`.pyc`ï¼‰ã‚’ã‚¯ãƒªã‚¢

### Q5: CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒåˆã‚ãªã„

A: **é‡è¦**: ãƒ›ã‚¹ãƒˆã®GPUãƒ‰ãƒ©ã‚¤ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã€ã‚³ãƒ³ãƒ†ãƒŠã®CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

| CUDA Version | æœ€å°ãƒ‰ãƒ©ã‚¤ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ |
|--------------|---------------------|
| CUDA 13.0    | 525.60.13           |
| CUDA 12.8    | 525.60.13           |
| CUDA 12.6    | 525.60.13           |
| CUDA 12.4    | 525.60.13           |
| CUDA 12.1    | 525.60.13           |
| CUDA 11.8    | 520.61.05           |

**ç¢ºèªæ–¹æ³•**:
```bash
nvidia-smi  # Driver Versionã‚’ç¢ºèª
```

### Q6: ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’å°ã•ãã—ãŸã„

A: æœ¬ç•ªç’°å¢ƒï¼ˆDockerfile.prodï¼‰ã¯æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã•ã‚‰ã«å‰Šæ¸›ã™ã‚‹å ´åˆï¼š
- ä¸è¦ãªä¾å­˜é–¢ä¿‚ã‚’ `pyproject.toml` ã‹ã‚‰å‰Šé™¤
- ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰ã§ä¸è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‰Šé™¤
- `.dockerignore` ã‚’è¦‹ç›´ã—ã¦ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
- `nvidia/cuda:{{ cuda_version }}-cudnn{{ cudnn_version.split('.')[0] }}-runtime-ubuntu22.04`ï¼ˆruntimeã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰ã‚’ä½¿ç”¨ï¼ˆdevelã§ã¯ãªãï¼‰

### Q7: è¤‡æ•°GPUã§ã®åˆ†æ•£å­¦ç¿’

A: PyTorch DistributedDataParallel (DDP) ã‚’ä½¿ç”¨ï¼š

```python
# train.py
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆæœŸåŒ–
dist.init_process_group(backend='nccl')
model = DDP(model, device_ids=[local_rank])
```

```bash
# docker-compose.ymlã§å…¨GPUä½¿ç”¨ã‚’ç¢ºèª
docker compose run --rm dev torchrun --nproc_per_node=2 train.py
```

### Q8: Jupyter Labã«æ¥ç¶šã§ããªã„

A: ä»¥ä¸‹ã‚’ç¢ºèªï¼š

```bash
# ãƒãƒ¼ãƒˆ8888ã§Jupyterèµ·å‹•
docker compose run --rm -p 8888:8888 dev jupyter lab --ip=0.0.0.0 --no-browser

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
http://localhost:8888
```

ãƒˆãƒ¼ã‚¯ãƒ³ã¯èµ·å‹•æ™‚ã®ãƒ­ã‚°ã«è¡¨ç¤ºã•ã‚Œã¾ã™ï¼š
```
http://127.0.0.1:8888/lab?token=abc123...
```

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [Dockerå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.docker.com/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- [PyTorch Docker Hub](https://hub.docker.com/r/pytorch/pytorch)
- [NVIDIA CUDA Docker Hub](https://hub.docker.com/r/nvidia/cuda)
- [uv Dockerçµ±åˆã‚¬ã‚¤ãƒ‰](https://docs.astral.sh/uv/guides/integration/docker/)
{% if use_nix %}
- [Nixç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](NIX_SETUP.md)
{% endif %}
- [CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](CUDA_SETUP.md)

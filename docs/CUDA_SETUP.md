# CUDA/PyTorchç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€`uv-torch-nix-template`ã‚’ä½¿ç”¨ã—ãŸPyTorch/CUDAé–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ¯ æ¦‚è¦

ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã€Nixã‚’ä½¿ç”¨ã—ã¦CUDAç’°å¢ƒã‚’å®Œå…¨ã«ç®¡ç†ã—ã€å†ç¾å¯èƒ½ãªPyTorché–‹ç™ºç’°å¢ƒã‚’æä¾›ã—ã¾ã™ã€‚

### ä¸»ãªç‰¹å¾´

- **å®Œå…¨ãªå†ç¾æ€§**: CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€cuDNNã€PyTorchã‚’å›ºå®š
- **ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ç®¡ç†**: Nixã«ã‚ˆã‚‹ä¾å­˜é–¢ä¿‚ã®å®Œå…¨ãªç®¡ç†
- **direnvçµ±åˆ**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å…¥ã‚‹ã¨è‡ªå‹•ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆ

ã¾ãšã€`uv-nix-template`ã§ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ï¼š

```bash
uvx copier copy --trust gh:nishide-dev/uv-nix-template my-torch-project
cd my-torch-project
```

### 2. PyTorch/CUDAæ‹¡å¼µã‚’é©ç”¨

æ¬¡ã«ã€ã“ã®PyTorchæ‹¡å¼µãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã—ã¾ã™ï¼š

```bash
uvx copier copy --trust gh:nishide-dev/uv-torch-nix-template .
```

å¯¾è©±çš„ã«ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¾ã™ï¼š

- **PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³**: ä¾‹: `2.5.1`, `2.4.1`
- **CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³**: ä¾‹: `12.4`, `12.1`, `11.8`
- **cuDNNãƒãƒ¼ã‚¸ãƒ§ãƒ³**: ä¾‹: `9.1.0`, `8.9.7`
- **PyTorch CUDA architecture**: ä¾‹: `cu124`, `cu121`ï¼ˆç©ºç™½ã§è‡ªå‹•ç”Ÿæˆï¼‰
- **torchvision**: å¿…è¦ã«å¿œã˜ã¦ `yes`
- **torchaudio**: å¿…è¦ã«å¿œã˜ã¦ `yes`

### 3. Nixç’°å¢ƒã‚’æ§‹ç¯‰

```bash
# direnvã‚’è¨±å¯ï¼ˆåˆå›ã®ã¿ï¼‰
direnv allow

# ã¾ãŸã¯æ‰‹å‹•ã§Nixç’°å¢ƒã«å…¥ã‚‹
nix develop
```

### 4. PyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

CUDAå¯¾å¿œPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š

```bash
# PyTorch + torchvisionã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆCUDA 12.4ã®ä¾‹ï¼‰
uv add torch==2.5.1+cu124 torchvision==0.20.1+cu124 --index-url https://download.pytorch.org/whl/cu124

# ã¾ãŸã¯ã€CUDA 12.1ã®å ´åˆ
uv add torch==2.5.1+cu121 torchvision==0.20.1+cu121 --index-url https://download.pytorch.org/whl/cu121

# torchaudioã‚‚å¿…è¦ãªå ´åˆ
uv add torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124
```

**æ³¨æ„**: PyTorchã®CUDAãƒ“ãƒ«ãƒ‰ã¯å…¬å¼ã®PyPIã§ã¯ãªãã€PyTorchã®å°‚ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### 5. å‹•ä½œç¢ºèª

```bash
# PyTorchã¨CUDAã®ç¢ºèª
uv run python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA version: {torch.version.cuda}')
print(f'cuDNN version: {torch.backends.cudnn.version()}')
if torch.cuda.is_available():
    print(f'GPU device: {torch.cuda.get_device_name(0)}')
"
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ï¼š

```
PyTorch version: 2.5.1+cu124
CUDA available: True
CUDA version: 12.4
cuDNN version: 9001
GPU device: NVIDIA GeForce RTX 4090
```

## ğŸ“¦ ä¾å­˜é–¢ä¿‚ã®ç®¡ç†

### ä¸€èˆ¬çš„ãªPyTorchãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ 

```bash
# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç³»
uv add numpy pandas scikit-learn matplotlib seaborn

# Deep Learningç³»
uv add transformers accelerate datasets

# ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ç³»
uv add opencv-python pillow albumentations

# é–‹ç™ºãƒ„ãƒ¼ãƒ«
uv add --dev jupyter ipython tensorboard
```

### PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ›´æ–°

```bash
# æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
uv add torch==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124
```

## ğŸ”§ Nixã«ã‚ˆã‚‹ç’°å¢ƒã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å¤‰æ›´

`flake.nix`ã‚’ç·¨é›†ã—ã¦CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¤‰æ›´ã§ãã¾ã™ï¼š

```nix
# ä¾‹: CUDA 12.1ã«å¤‰æ›´
cudaVersion = "12_1";
cudaPackages = pkgs.cudaPackages_12_1;
```

å¤‰æ›´å¾Œã€ç’°å¢ƒã‚’å†æ§‹ç¯‰ï¼š

```bash
nix flake update
direnv reload
```

### è¿½åŠ ã®CUDAãƒ©ã‚¤ãƒ–ãƒ©ãƒª

`flake.nix`ã®`cudaLibs`ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ ï¼š

```nix
cudaLibs = with cudaPackages; [
  cuda_cudart
  cuda_nvcc
  cudnn
  nccl          # è¿½åŠ : åˆ†æ•£å­¦ç¿’ç”¨
  cutlass       # è¿½åŠ : CUDA C++ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
];
```

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA not available

**ç—‡çŠ¶**: `torch.cuda.is_available()`ãŒ`False`ã‚’è¿”ã™

**åŸå› ã¨å¯¾å‡¦**:

1. **GPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ç¢ºèª**
   ```bash
   nvidia-smi
   ```
   æ­£ã—ãè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã€NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

2. **CUDAäº’æ›æ€§ã®ç¢ºèª**
   - ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®äº’æ›æ€§ã‚’ç¢ºèª
   - CUDA 12.4ã¯ãƒ‰ãƒ©ã‚¤ãƒãƒ¼525.60.13ä»¥ä¸ŠãŒå¿…è¦

3. **PyTorchãƒ“ãƒ«ãƒ‰ã®ç¢ºèª**
   ```bash
   uv run python -c "import torch; print(torch.version.cuda)"
   ```
   `None`ã®å ´åˆã€CPUç‰ˆãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™

### LD_LIBRARY_PATH ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `libcudart.so.12` ãªã©ãŒè¦‹ã¤ã‹ã‚‰ãªã„

**å¯¾å‡¦**:

```bash
# Nixç’°å¢ƒå†…ã§å®Ÿè¡Œ
echo $LD_LIBRARY_PATH

# ãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€direnvã‚’å†èª­ã¿è¾¼ã¿
direnv reload
```

### Out of Memory (OOM)

**ç—‡çŠ¶**: GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**å¯¾å‡¦**:

```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
batch_size = 16  # â†’ 8

# æ··åˆç²¾åº¦å­¦ç¿’ã‚’ä½¿ç”¨
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(input)
```

### Nixãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `nix develop`ã§ã‚¨ãƒ©ãƒ¼

**å¯¾å‡¦**:

1. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢**
   ```bash
   nix-collect-garbage
   ```

2. **Flakeã‚’æ›´æ–°**
   ```bash
   nix flake update
   ```

3. **unfreeãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è¨±å¯ç¢ºèª**
   `flake.nix`ã§`allowUnfree = true`ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### cuDNN Benchmark

```python
import torch
torch.backends.cudnn.benchmark = True  # æœ€åˆã®å®Ÿè¡Œã¯é…ã„ãŒã€ä»¥é™é«˜é€ŸåŒ–
```

### ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æœ€é©åŒ–

```python
from torch.utils.data import DataLoader

loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,      # CPUã‚³ã‚¢æ•°ã«å¿œã˜ã¦èª¿æ•´
    pin_memory=True,    # GPUè»¢é€ã‚’é«˜é€ŸåŒ–
    prefetch_factor=2,  # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒãƒƒãƒ•ã‚¡
)
```

### æ··åˆç²¾åº¦å­¦ç¿’

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in loader:
    optimizer.zero_grad()

    with autocast():
        output = model(data)
        loss = criterion(output, target)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯

- [PyTorchå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorch.org/docs/stable/index.html)
- [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/)
- [Nix CUDA Support](https://nixos.wiki/wiki/CUDA)
- [uv Documentation](https://docs.astral.sh/uv/)

## ğŸ’¡ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®š**: `uv.lock`ã§ä¾å­˜é–¢ä¿‚ã‚’å›ºå®šã—ã€ãƒãƒ¼ãƒ å…¨ä½“ã§åŒã˜ç’°å¢ƒã‚’å…±æœ‰
2. **GPUç›£è¦–**: `nvidia-smi`ã‚„`nvtop`ã§GPUä½¿ç”¨ç‡ã‚’ç›£è¦–
3. **å®Ÿé¨“ç®¡ç†**: Weights & Biasesã‚„MLflowã§å®Ÿé¨“ã‚’è¿½è·¡
4. **å‹ãƒã‚§ãƒƒã‚¯**: `uv run ty check`ã§å‹å®‰å…¨æ€§ã‚’ç¢ºä¿
5. **ãƒ†ã‚¹ãƒˆ**: GPUã‚³ãƒ¼ãƒ‰ã‚‚pytestã§ãƒ†ã‚¹ãƒˆå¯èƒ½

```python
import pytest
import torch

@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
def test_cuda_operation():
    x = torch.randn(100, 100).cuda()
    y = torch.matmul(x, x)
    assert y.is_cuda
```
